behaviors:
  Drone:
    trainer_type: ppo
    keep_checkpoints: 5
    max_steps: 15000000
    time_horizon: 128 # Trade off between a less biased, but higher variance estimate (long time horizon) and more biased, but less varied estimate (short time horizon)
    summary_freq: 20000
    threaded: true
    
    hyperparameters:
      batch_size: 256 # Number of experiences in each iteration of gradient descent. This should always be multiple times smaller than buffer_size
      buffer_size: 10240
      learning_rate: 0.0003
      beta: 0.005 # (Typical range: 1e-4 - 1e-2) If entropy drops too quickly, increase beta. If entropy drops too slowly, decrease beta
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3
      learning_rate_schedule: linear
      
    network_settings:
      normalize: false
      hidden_units: 256
      num_layers: 2
      vis_encode_type: simple
      
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0

environment_parameters:
  difficulty_level:
    curriculum:
      - name: Lesson0 # The '-' is important as this is a list
        completion_criteria:
          measure: reward
          behavior: Drone
          signal_smoothing: true
          min_lesson_length: 100
          threshold: 1.0
        value: 1.0
        
      - name: Lesson1 # This is the start of the second lesson
        completion_criteria:
          measure: reward
          behavior: Drone
          signal_smoothing: true
          min_lesson_length: 100
          threshold: 1.0
        value: 2.0
        
      - name: Lesson2
        value: 3.0
